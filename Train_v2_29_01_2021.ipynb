{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_v2_29/01/2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiraNdmsJn+jVEjTr+zXOk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-Khetani/Dino-RL/blob/main/Train_v2_29_01_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG6cbM2aqu5z",
        "outputId": "fde022c7-3fae-4dd7-f25e-b172297e2ee6"
      },
      "source": [
        "!git clone https://github.com/Nikhil-Khetani/Dino-RL.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Dino-RL' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxSqi4axxaMx",
        "outputId": "4c22a009-fc80-478f-9626-9badb60af7ab"
      },
      "source": [
        "!git clone https://github.com/qfettes/DeepRL-Tutorials.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DeepRL-Tutorials' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UYFzybQrKh8"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "#sys.path.append(os.path.join(\"content\",\"Dino-RL\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBrDvu3srtAq",
        "outputId": "93ee8fcd-c1cd-4bc0-d4a6-017fa696abed"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCTJAEBVyFjY",
        "outputId": "884569fc-3e9e-405e-a445-070ec65fb7ae"
      },
      "source": [
        "!pip install baselines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: baselines in /usr/local/lib/python3.6/dist-packages (0.1.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines) (1.0.0)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.6/dist-packages (from baselines) (3.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines) (1.3.0)\n",
            "Requirement already satisfied: gym[atari,classic_control,mujoco,robotics] in /usr/local/lib/python3.6/dist-packages (from baselines) (0.17.3)\n",
            "Requirement already satisfied: tensorflow>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from baselines) (2.4.0)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from baselines) (3.38.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from baselines) (0.3.3)\n",
            "Requirement already satisfied: zmq in /usr/local/lib/python3.6/dist-packages (from baselines) (0.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (1.19.5)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (4.1.2.30)\n",
            "Requirement already satisfied: imageio; extra == \"mujoco\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (2.4.1)\n",
            "Collecting mujoco-py<2.0,>=1.50; extra == \"mujoco\"\n",
            "  Using cached https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (0.36.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.12.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (2.10.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (0.10.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->baselines) (2.4.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from zmq->baselines) (20.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control,mujoco,robotics]->baselines) (0.16.0)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"mujoco\"->gym[atari,classic_control,mujoco,robotics]->baselines) (2.0.0)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"mujoco\"->gym[atari,classic_control,mujoco,robotics]->baselines) (0.29.21)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"mujoco\"->gym[atari,classic_control,mujoco,robotics]->baselines) (1.14.4)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"mujoco\"->gym[atari,classic_control,mujoco,robotics]->baselines) (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=1.4.0->baselines) (51.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.3.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"mujoco\"->gym[atari,classic_control,mujoco,robotics]->baselines) (2.20)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.4.0)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-26p_gg3r/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-26p_gg3r/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-_x_2f06b/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTrhztyFrRcX",
        "outputId": "5c6064ba-2c7c-4187-fe72-bd7d7167cc71"
      },
      "source": [
        "import os\r\n",
        "import pygame\r\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 2.0.1 (SDL 2.0.14, Python 3.6.9)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hazyiLq5sK6m"
      },
      "source": [
        "import game"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8d7UkUmtUlc",
        "outputId": "eda4880e-7f2b-4be5-fb28-6c566138df8f"
      },
      "source": [
        "sys.path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tl_8MV_xJs4"
      },
      "source": [
        "import random, pickle, os.path, math, glob\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from timeit import default_timer as timer\r\n",
        "from datetime import timedelta\r\n",
        "from timeit import default_timer as timer\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import matplotlib\r\n",
        "%matplotlib inline\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from utils.wrappers import *\r\n",
        "from utils.hyperparameters import Config\r\n",
        "from utils.plot import plot_all_data\r\n",
        "\r\n",
        "from agents.BaseAgent import BaseAgent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZXIc6HQxQus"
      },
      "source": [
        "config = Config()\r\n",
        "\r\n",
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "#epsilon variables\r\n",
        "config.epsilon_start    = 1.0\r\n",
        "config.epsilon_final    = 0.01\r\n",
        "config.epsilon_decay    = 30000\r\n",
        "config.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\r\n",
        "\r\n",
        "#misc agent variables\r\n",
        "config.GAMMA = 0.99\r\n",
        "config.LR    = 1e-4\r\n",
        "\r\n",
        "#memory\r\n",
        "config.TARGET_NET_UPDATE_FREQ = 1000\r\n",
        "config.EXP_REPLAY_SIZE        = 100000\r\n",
        "config.BATCH_SIZE             = 32\r\n",
        "\r\n",
        "#Learning control variables\r\n",
        "config.LEARN_START = 10000\r\n",
        "config.MAX_FRAMES  = 1000000\r\n",
        "config.UPDATE_FREQ = 1\r\n",
        "\r\n",
        "#data logging parameters\r\n",
        "config.ACTION_SELECTION_COUNT_FREQUENCY = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1quM8WKfyePe"
      },
      "source": [
        "class ExperienceReplayMemory:\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.capacity = capacity\r\n",
        "        self.memory = []\r\n",
        "\r\n",
        "    def push(self, transition):\r\n",
        "        self.memory.append(transition)\r\n",
        "        if len(self.memory) > self.capacity:\r\n",
        "            del self.memory[0]\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        return random.sample(self.memory, batch_size), None, None\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.memory)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGmmS049yhfp"
      },
      "source": [
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, input_shape, num_actions):\r\n",
        "        super(DQN, self).__init__()\r\n",
        "        \r\n",
        "        self.input_shape = input_shape\r\n",
        "        self.num_actions = num_actions\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4)\r\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\r\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(self.feature_size(), 512)\r\n",
        "        self.fc2 = nn.Linear(512, self.num_actions)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.conv1(x))\r\n",
        "        x = F.relu(self.conv2(x))\r\n",
        "        x = F.relu(self.conv3(x))\r\n",
        "        x = x.view(x.size(0), -1)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = self.fc2(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "    \r\n",
        "    def feature_size(self):\r\n",
        "        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEk2MtCNyw2z"
      },
      "source": [
        "class Model(BaseAgent):\r\n",
        "    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\r\n",
        "        super(Model, self).__init__(config=config, env=env, log_dir=log_dir)\r\n",
        "        self.device = config.device\r\n",
        "\r\n",
        "        self.gamma = config.GAMMA\r\n",
        "        self.lr = config.LR\r\n",
        "        self.target_net_update_freq = config.TARGET_NET_UPDATE_FREQ\r\n",
        "        self.experience_replay_size = config.EXP_REPLAY_SIZE\r\n",
        "        self.batch_size = config.BATCH_SIZE\r\n",
        "        self.learn_start = config.LEARN_START\r\n",
        "        self.update_freq = config.UPDATE_FREQ\r\n",
        "\r\n",
        "        self.static_policy = static_policy\r\n",
        "        self.num_feats = env.observation_space.shape\r\n",
        "        self.num_actions = 2\r\n",
        "        self.env = env\r\n",
        "\r\n",
        "        self.declare_networks()\r\n",
        "            \r\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\r\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\r\n",
        "        \r\n",
        "        #move to correct device\r\n",
        "        self.model = self.model.to(self.device)\r\n",
        "        self.target_model.to(self.device)\r\n",
        "\r\n",
        "        if self.static_policy:\r\n",
        "            self.model.eval()\r\n",
        "            self.target_model.eval()\r\n",
        "        else:\r\n",
        "            self.model.train()\r\n",
        "            self.target_model.train()\r\n",
        "\r\n",
        "        self.update_count = 0\r\n",
        "\r\n",
        "        self.declare_memory()\r\n",
        "\r\n",
        "    def declare_networks(self):\r\n",
        "        self.model = DQN(self.num_feats, self.num_actions)\r\n",
        "        self.target_model = DQN(self.num_feats, self.num_actions)\r\n",
        "\r\n",
        "    def declare_memory(self):\r\n",
        "        self.memory = ExperienceReplayMemory(self.experience_replay_size)\r\n",
        "\r\n",
        "    def append_to_replay(self, s, a, r, s_):\r\n",
        "        self.memory.push((s, a, r, s_))\r\n",
        "\r\n",
        "    def prep_minibatch(self):\r\n",
        "        # random transition batch is taken from experience replay memory\r\n",
        "        transitions, indices, weights = self.memory.sample(self.batch_size)\r\n",
        "        \r\n",
        "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\r\n",
        "\r\n",
        "        shape = (-1,)+self.num_feats\r\n",
        "\r\n",
        "        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\r\n",
        "        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).squeeze().view(-1, 1)\r\n",
        "        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).squeeze().view(-1, 1)\r\n",
        "        \r\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\r\n",
        "        try: #sometimes all next states are false\r\n",
        "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).view(shape)\r\n",
        "            empty_next_state_values = False\r\n",
        "        except:\r\n",
        "            non_final_next_states = None\r\n",
        "            empty_next_state_values = True\r\n",
        "\r\n",
        "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights\r\n",
        "\r\n",
        "    def compute_loss(self, batch_vars):\r\n",
        "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\r\n",
        "\r\n",
        "        #estimate\r\n",
        "        current_q_values = self.model(batch_state).gather(1, batch_action)\r\n",
        "        \r\n",
        "        #target\r\n",
        "        with torch.no_grad():\r\n",
        "            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\r\n",
        "            if not empty_next_state_values:\r\n",
        "                max_next_action = self.get_max_next_state_action(non_final_next_states)\r\n",
        "                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\r\n",
        "            expected_q_values = batch_reward + self.gamma*max_next_q_values\r\n",
        "\r\n",
        "        diff = (expected_q_values - current_q_values)\r\n",
        "        loss = self.MSE(diff)\r\n",
        "        loss = loss.mean()\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def update(self, s, a, r, s_, frame=0):\r\n",
        "        if self.static_policy:\r\n",
        "            return None\r\n",
        "\r\n",
        "        self.append_to_replay(s, a, r, s_)\r\n",
        "\r\n",
        "        if frame < self.learn_start or frame % self.update_freq != 0:\r\n",
        "            return None\r\n",
        "\r\n",
        "        batch_vars = self.prep_minibatch()\r\n",
        "\r\n",
        "        loss = self.compute_loss(batch_vars)\r\n",
        "\r\n",
        "        # Optimize the model\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        for param in self.model.parameters():\r\n",
        "            param.grad.data.clamp_(-1, 1)\r\n",
        "        self.optimizer.step()\r\n",
        "\r\n",
        "        self.update_target_model()\r\n",
        "        self.save_td(loss.item(), frame)\r\n",
        "        self.save_sigma_param_magnitudes(frame)\r\n",
        "\r\n",
        "    def get_action(self, s, eps=0.1):\r\n",
        "        with torch.no_grad():\r\n",
        "            if np.random.random() >= eps or self.static_policy:\r\n",
        "                X = torch.tensor([s], device=self.device, dtype=torch.float)\r\n",
        "                a = self.model(X).max(1)[1].view(1, 1)\r\n",
        "                return a.item()\r\n",
        "            else:\r\n",
        "                return np.random.randint(0, self.num_actions)\r\n",
        "\r\n",
        "    def update_target_model(self):\r\n",
        "        self.update_count+=1\r\n",
        "        self.update_count = self.update_count % self.target_net_update_freq\r\n",
        "        if self.update_count == 0:\r\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\r\n",
        "\r\n",
        "    def get_max_next_state_action(self, next_states):\r\n",
        "        return self.target_model(next_states).max(dim=1)[1].view(-1, 1)\r\n",
        "\r\n",
        "    def finish_nstep(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def reset_hx(self):\r\n",
        "        pass\r\n",
        "    \r\n",
        "    def MSE(self, x):\r\n",
        "        return 0.5 * x.pow(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "5i1IYj64y29n",
        "outputId": "2d0b9601-4312-4c3f-df64-d786f28d6ebb"
      },
      "source": [
        "start=timer()\r\n",
        "\r\n",
        "log_dir = \"/tmp/gym/\"\r\n",
        "try:\r\n",
        "    os.makedirs(log_dir)\r\n",
        "except OSError:\r\n",
        "    files = glob.glob(os.path.join(log_dir, '*.monitor.csv')) \\\r\n",
        "        + glob.glob(os.path.join(log_dir, '*td.csv')) \\\r\n",
        "        + glob.glob(os.path.join(log_dir, '*sig_param_mag.csv')) \\\r\n",
        "        + glob.glob(os.path.join(log_dir, '*action_log.csv'))\r\n",
        "    for f in files:\r\n",
        "        os.remove(f)\r\n",
        "\r\n",
        "env = game.DinoGame(None,400,400)\r\n",
        "model  = Model(env=env, config=config, log_dir=log_dir)\r\n",
        "\r\n",
        "episode_reward = 0\r\n",
        "\r\n",
        "observation = env.reset()\r\n",
        "for frame_idx in range(1, config.MAX_FRAMES + 1):\r\n",
        "    print(frame_idx)\r\n",
        "    epsilon = config.epsilon_by_frame(frame_idx)\r\n",
        "\r\n",
        "    action = model.get_action(observation, epsilon)\r\n",
        "    model.save_action(action, frame_idx) #log action selection\r\n",
        "\r\n",
        "    prev_observation=observation\r\n",
        "    observation, reward, done, _ = env.nextframe(action)\r\n",
        "    observation = None if done else observation\r\n",
        "\r\n",
        "    model.update(prev_observation, action, reward, observation, frame_idx)\r\n",
        "    episode_reward += reward\r\n",
        "\r\n",
        "    if done:\r\n",
        "        model.finish_nstep()\r\n",
        "        model.reset_hx()\r\n",
        "        observation = env.reset()\r\n",
        "        model.save_reward(episode_reward)\r\n",
        "        episode_reward = 0\r\n",
        "\r\n",
        "    if frame_idx % 10000 == 0:\r\n",
        "        model.save_w()\r\n",
        "        try:\r\n",
        "            clear_output(True)\r\n",
        "            plot_all_data(log_dir, env_id, 'DQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)\r\n",
        "        except IOError:\r\n",
        "            pass\r\n",
        "\r\n",
        "model.save_w()\r\n",
        "env.close()\r\n",
        "plot_all_data(log_dir, env_id, 'DQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-742475babdf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDinoGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-3c0a245d7040>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, static_policy, env, config, log_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatic_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DinoGame' object has no attribute 'observation_space'"
          ]
        }
      ]
    }
  ]
}