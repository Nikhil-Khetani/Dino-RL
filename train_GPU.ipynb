{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9e/sbXcUajLwRA/hfMGdQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-Khetani/Dino-RL/blob/main/train_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VkFrzP7Dau8",
        "outputId": "71f9bf04-3a1f-4d30-d8ab-85f4f85fb953"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/da/4ff439558641a26dd29b04c25947e6c0ace041f56b2aa2ef1134edab06b8/pygame-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 271kB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkoe3seEDgUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472b256a-577e-4967-9c19-b60c20c3d033"
      },
      "source": [
        "import cv2\r\n",
        "import time \r\n",
        "import os, sys\r\n",
        "import game\r\n",
        "import math\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "import pygame\r\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 2.0.1 (SDL 2.0.14, Python 3.6.9)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuTd3rk_Dmct"
      },
      "source": [
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkpZyQ-9Dq48"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "DISPLAY_HEIGHT=400\r\n",
        "DISPLAY_WIDTH=400\r\n",
        "STATE_HEIGHT = DISPLAY_HEIGHT-1\r\n",
        "STATE_WIDTH = DISPLAY_WIDTH-1\r\n",
        "\r\n",
        "pygame.init()\r\n",
        "\r\n",
        "image_size=84\r\n",
        "batch_size=32\r\n",
        "lr=1e-6\r\n",
        "gamma=0.99\r\n",
        "initial_epsilon=0.1\r\n",
        "final_epsilon=1e-4\r\n",
        "num_iters=2000000\r\n",
        "replay_memory_size=50000"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAC26h3MFwMq"
      },
      "source": [
        "\r\n",
        "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\r\n",
        "\r\n",
        "def pre_processing(image, width, height):\r\n",
        "    image = cv2.cvtColor(cv2.resize(image, (width, height)), cv2.COLOR_BGR2GRAY)\r\n",
        "    _, image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY)\r\n",
        "    return image[None, :, :].astype(np.float32)\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkoOWRKMFV71"
      },
      "source": [
        "\r\n",
        "class DeepQNetwork(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(DeepQNetwork, self).__init__()\r\n",
        "\r\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(inplace=True))\r\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(inplace=True))\r\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(inplace=True))\r\n",
        "\r\n",
        "        self.fc1 = nn.Sequential(nn.Linear(7 * 7 * 64, 512), nn.ReLU(inplace=True))\r\n",
        "        self.fc2 = nn.Linear(512, 2)\r\n",
        "        self._create_weights()\r\n",
        "\r\n",
        "    def _create_weights(self):\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\r\n",
        "                nn.init.uniform(m.weight, -0.01, 0.01)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        output = self.conv1(input)\r\n",
        "        output = self.conv2(output)\r\n",
        "        output = self.conv3(output)\r\n",
        "        output = output.view(output.size(0), -1)\r\n",
        "        output = self.fc1(output)\r\n",
        "        output = self.fc2(output)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ZgrRJgFcFM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01e8f806-f817-435a-94dd-f79ff79ef4e3"
      },
      "source": [
        "\r\n",
        "def train(episodes):\r\n",
        "    real_time_start = time.time()\r\n",
        "    CPU_time_start = time.process_time()\r\n",
        "    model = DeepQNetwork()\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\r\n",
        "    criterion = torch.nn.MSELoss()\r\n",
        "    current_game = game.DinoGame(None,400,400)\r\n",
        "    image, reward, endgame = current_game.nextframe(0)\r\n",
        "    \r\n",
        "    image = pre_processing(image, 84,84)\r\n",
        "    #image = torch.from_numpy(np.expand_dims(np.transpose(image,(2,0,1)),0))\r\n",
        "    image = torch.from_numpy(image)\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        model.cuda()\r\n",
        "        image = image.cuda()\r\n",
        "    #image=image.float()\r\n",
        "    state=torch.cat(tuple(image for _ in range (4)))[None, :, :, :]\r\n",
        "    #print(state.shape)\r\n",
        "    replay_memory = []\r\n",
        "    episode = 0\r\n",
        "    checkpoint_real_time_elapsed = 0\r\n",
        "    checkpoint_CPU_time_elapsed = 0\r\n",
        "\r\n",
        "\r\n",
        "    for i in reversed(range(100)):\r\n",
        "      print(i)\r\n",
        "      checkpoint_path = \"checkpoint_{}.pt\".format(i*1000)\r\n",
        "      if os.path.exists(checkpoint_path):\r\n",
        "          checkpoint = torch.load(checkpoint_path)\r\n",
        "          model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "          optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "          episode = checkpoint['checkpoint_episode']\r\n",
        "          loss = checkpoint['loss']\r\n",
        "          checkpoint_real_time_elapsed = checkpoint['real_time_elapsed']\r\n",
        "          checkpoint_CPU_time_elapsed = checkpoint['CPU_time_elapsed']\r\n",
        "          print(\"checkpoint loaded\")\r\n",
        "          print(\"checkpoint episode:\" + str(episode))\r\n",
        "          break\r\n",
        "    \r\n",
        "    while episode<episodes:\r\n",
        "        pred = model(state)[0]\r\n",
        "        epsilon = final_epsilon+((episodes-episode)*(initial_epsilon-final_epsilon)/episodes)\r\n",
        "        take_random_action = random.random()<=epsilon\r\n",
        "        if take_random_action:\r\n",
        "            print('random')\r\n",
        "            action = random.randint(0,1)\r\n",
        "        else:\r\n",
        "            action=torch.argmax(pred)\r\n",
        "\r\n",
        "        next_image, reward, endgame = current_game.nextframe(action)\r\n",
        "        #next_image = torch.from_numpy(np.expand_dims(np.transpose(next_image,(2,0,1)),0))\r\n",
        "        #next_image = next_image.float()\r\n",
        "        #next_state = torch.cat((state.squeeze(0)[3:,:,:], next_image))\r\n",
        "\r\n",
        "        next_image = pre_processing(next_image, 84,84)\r\n",
        "\r\n",
        "        #action = action.unsqueeze(0)\r\n",
        "        #reward = torch.from_numpy(np.array([reward],dtype=np.float32)).unsqueeze(0)\r\n",
        "        next_image = torch.from_numpy(next_image)\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            next_image = next_image.cuda()\r\n",
        "        next_state = torch.cat((state[0, 1:, :, :], next_image))[None, :, :, :]\r\n",
        "\r\n",
        "        replay_memory.append([state, action, reward, next_state, endgame])\r\n",
        "        if len(replay_memory) > replay_memory_size:\r\n",
        "            del replay_memory[0]\r\n",
        "        batch = random.sample(replay_memory, min(len(replay_memory), batch_size))\r\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = zip(*batch)\r\n",
        "        state_batch = torch.cat(tuple(state for state in state_batch))\r\n",
        "        action_batch = torch.from_numpy(\r\n",
        "            np.array([[1, 0] if action == 0 else [0, 1] for action in action_batch], dtype=np.float32))\r\n",
        "        reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None])\r\n",
        "        next_state_batch = torch.cat(tuple(state for state in next_state_batch))\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            state_batch = state_batch.cuda()\r\n",
        "            action_batch = action_batch.cuda()\r\n",
        "            reward_batch = reward_batch.cuda()\r\n",
        "            next_state_batch = next_state_batch.cuda()\r\n",
        "        \r\n",
        "        current_prediction_batch = model(state_batch)\r\n",
        "        next_prediction_batch = model(next_state_batch)\r\n",
        "\r\n",
        "        y_batch = torch.cat(\r\n",
        "            tuple(reward if terminal else reward + gamma * torch.max(prediction) for reward, terminal, prediction in\r\n",
        "                  zip(reward_batch, terminal_batch, next_prediction_batch)))\r\n",
        "        q_value = torch.sum(current_prediction_batch * action_batch, dim=1)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(q_value, y_batch)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        state = next_state\r\n",
        "        \r\n",
        "        if episode %50 == 0:\r\n",
        "            print(\"Episode: {}/{}, Action: {}, Loss: {}, Epsilon {}, Reward: {}, Q-value: {}\".format(\r\n",
        "            episode + 1, episodes, action, loss, epsilon, reward, torch.max(pred)))\r\n",
        "        episode+=1\r\n",
        "\r\n",
        "        if episode % 5000 == 0:\r\n",
        "            real_time_elapsed = time.time()-real_time_start + checkpoint_real_time_elapsed\r\n",
        "            CPU_time_elapsed =time.process_time()-CPU_time_start + checkpoint_CPU_time_elapsed\r\n",
        "            print(\"Real time elapsed : {}, CPU time elapsed : {}\".format(real_time_elapsed,CPU_time_elapsed))\r\n",
        "            checkpoint_path = \"checkpoint_{}.pt\".format(episode)\r\n",
        "            torch.save({\r\n",
        "            'checkpoint_episode': episode,\r\n",
        "            'model_state_dict': model.state_dict(),\r\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "            'loss':loss,\r\n",
        "            'real_time_elapsed' : real_time_elapsed,\r\n",
        "            'CPU_time_elapsed' : CPU_time_elapsed\r\n",
        "            }, checkpoint_path)\r\n",
        "\r\n",
        "\r\n",
        "train(1000000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 665151/1000000, Action: 0, Loss: 0.08220286667346954, Epsilon 0.033551515000000004, Reward: 0.3, Q-value: 0.602920651435852\n",
            "random\n",
            "Game Over!\n",
            "random\n",
            "Episode: 665201/1000000, Action: 0, Loss: 0.08468332886695862, Epsilon 0.03354652000000001, Reward: 0.3, Q-value: 0.6109328866004944\n",
            "Episode: 665251/1000000, Action: 0, Loss: 0.0763884112238884, Epsilon 0.033541525, Reward: 0.3, Q-value: 0.6543446183204651\n",
            "random\n",
            "random\n",
            "Episode: 665301/1000000, Action: 0, Loss: 0.0871361792087555, Epsilon 0.03353653, Reward: 0.3, Q-value: 0.8216013312339783\n",
            "Game Over!\n",
            "Episode: 665351/1000000, Action: 0, Loss: 0.08846026659011841, Epsilon 0.03353153500000001, Reward: 0.3, Q-value: 0.8589000701904297\n",
            "Episode: 665401/1000000, Action: 0, Loss: 0.08507893979549408, Epsilon 0.03352654, Reward: 0.3, Q-value: 0.5809853672981262\n",
            "Episode: 665451/1000000, Action: 0, Loss: 0.08429461717605591, Epsilon 0.033521545, Reward: 0.3, Q-value: 0.537213146686554\n",
            "Episode: 665501/1000000, Action: 1, Loss: 0.08732911944389343, Epsilon 0.033516550000000006, Reward: 0.2, Q-value: 0.5387542247772217\n",
            "random\n",
            "random\n",
            "random\n",
            "Episode: 665551/1000000, Action: 0, Loss: 0.08663830906152725, Epsilon 0.033511555000000005, Reward: 0.3, Q-value: 0.6131423711776733\n",
            "Game Over!\n",
            "Episode: 665601/1000000, Action: 0, Loss: 0.08437678962945938, Epsilon 0.03350656, Reward: 0.3, Q-value: 0.7009625434875488\n",
            "random\n",
            "Episode: 665651/1000000, Action: 0, Loss: 0.08771339058876038, Epsilon 0.033501565000000004, Reward: 0.3, Q-value: 0.8322058320045471\n",
            "random\n",
            "random\n",
            "random\n",
            "random\n",
            "Episode: 665701/1000000, Action: 0, Loss: 0.0803697407245636, Epsilon 0.03349657, Reward: 0.3, Q-value: 0.7914360165596008\n",
            "Episode: 665751/1000000, Action: 0, Loss: 0.08749236911535263, Epsilon 0.03349157500000001, Reward: 0.3, Q-value: 0.8317943215370178\n",
            "random\n",
            "random\n",
            "Game Over!\n",
            "Episode: 665801/1000000, Action: 0, Loss: 0.0836309865117073, Epsilon 0.03348658, Reward: 0, Q-value: 0.5504056811332703\n",
            "random\n",
            "random\n",
            "Episode: 665851/1000000, Action: 0, Loss: 0.0829172357916832, Epsilon 0.033481585, Reward: 0.3, Q-value: 0.5507650375366211\n",
            "random\n",
            "Episode: 665901/1000000, Action: 0, Loss: 0.07947446405887604, Epsilon 0.03347659000000001, Reward: 0.3, Q-value: 0.6235567927360535\n",
            "Game Over!\n",
            "Episode: 665951/1000000, Action: 0, Loss: 0.08077356219291687, Epsilon 0.03347159500000001, Reward: 0.3, Q-value: 0.804448127746582\n",
            "random\n",
            "Episode: 666001/1000000, Action: 0, Loss: 0.1144612580537796, Epsilon 0.0334666, Reward: 0.3, Q-value: 0.7689206600189209\n",
            "random\n",
            "random\n",
            "Episode: 666051/1000000, Action: 0, Loss: 0.10646194964647293, Epsilon 0.033461605000000005, Reward: 0.3, Q-value: 0.8923977613449097\n",
            "random\n",
            "random\n",
            "random\n",
            "random\n",
            "Episode: 666101/1000000, Action: 0, Loss: 0.08526033908128738, Epsilon 0.033456610000000005, Reward: 0.3, Q-value: 0.7278225421905518\n",
            "random\n",
            "random\n",
            "random\n",
            "random\n",
            "Game Over!\n",
            "Episode: 666151/1000000, Action: 0, Loss: 0.08367488533258438, Epsilon 0.033451615000000004, Reward: 0.3, Q-value: 0.5437678098678589\n",
            "random\n",
            "random\n",
            "random\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-338fa4ebea21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-338fa4ebea21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mcurrent_prediction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mnext_prediction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         y_batch = torch.cat(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6eb79bee22b5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    717\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 self._forward_pre_hooks.values()):\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}