{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlOjMPE0+gRtN88JIPWgwi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-Khetani/Dino-RL/blob/main/train_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VkFrzP7Dau8",
        "outputId": "9b1ae0c0-66e4-4710-ad20-feb5b9fafe14"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/da/4ff439558641a26dd29b04c25947e6c0ace041f56b2aa2ef1134edab06b8/pygame-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 14.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkoe3seEDgUh"
      },
      "source": [
        "import cv2\r\n",
        "import time \r\n",
        "import os, sys\r\n",
        "import game\r\n",
        "import math\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "import pygame\r\n",
        "import time"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuTd3rk_Dmct"
      },
      "source": [
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkpZyQ-9Dq48"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "DISPLAY_HEIGHT=400\r\n",
        "DISPLAY_WIDTH=400\r\n",
        "STATE_HEIGHT = DISPLAY_HEIGHT-1\r\n",
        "STATE_WIDTH = DISPLAY_WIDTH-1\r\n",
        "\r\n",
        "pygame.init()\r\n",
        "\r\n",
        "image_size=84\r\n",
        "batch_size=32\r\n",
        "lr=1e-6\r\n",
        "gamma=0.99\r\n",
        "initial_epsilon=0.1\r\n",
        "final_epsilon=1e-4\r\n",
        "num_iters=2000000\r\n",
        "replay_memory_size=50000"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAC26h3MFwMq"
      },
      "source": [
        "\r\n",
        "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\r\n",
        "\r\n",
        "def pre_processing(image, width, height):\r\n",
        "    image = cv2.cvtColor(cv2.resize(image, (width, height)), cv2.COLOR_BGR2GRAY)\r\n",
        "    _, image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY)\r\n",
        "    return image[None, :, :].astype(np.float32)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkoOWRKMFV71"
      },
      "source": [
        "\r\n",
        "class DeepQNetwork(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(DeepQNetwork, self).__init__()\r\n",
        "\r\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(inplace=True))\r\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(inplace=True))\r\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(inplace=True))\r\n",
        "\r\n",
        "        self.fc1 = nn.Sequential(nn.Linear(7 * 7 * 64, 512), nn.ReLU(inplace=True))\r\n",
        "        self.fc2 = nn.Linear(512, 2)\r\n",
        "        self._create_weights()\r\n",
        "\r\n",
        "    def _create_weights(self):\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\r\n",
        "                nn.init.uniform(m.weight, -0.01, 0.01)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        output = self.conv1(input)\r\n",
        "        output = self.conv2(output)\r\n",
        "        output = self.conv3(output)\r\n",
        "        output = output.view(output.size(0), -1)\r\n",
        "        output = self.fc1(output)\r\n",
        "        output = self.fc2(output)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ZgrRJgFcFM"
      },
      "source": [
        "\r\n",
        "def train(episodes):\r\n",
        "    real_time_start = time.time()\r\n",
        "    CPU_time_start = time.process_time()\r\n",
        "    model = DeepQNetwork()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\r\n",
        "    criterion = torch.nn.MSELoss()\r\n",
        "    current_game = game.DinoGame(None,400,400)\r\n",
        "    image, reward, endgame = current_game.nextframe(0)\r\n",
        "    \r\n",
        "    image = pre_processing(image, 84,84)\r\n",
        "    #image = torch.from_numpy(np.expand_dims(np.transpose(image,(2,0,1)),0))\r\n",
        "    image = torch.from_numpy(image)\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        model.cuda()\r\n",
        "        image = image.cuda()\r\n",
        "    #image=image.float()\r\n",
        "    state=torch.cat(tuple(image for _ in range (4)))[None, :, :, :]\r\n",
        "    #print(state.shape)\r\n",
        "    replay_memory = []\r\n",
        "    episode = 0\r\n",
        "    \r\n",
        "    while episode<episodes:\r\n",
        "        pred = model(state)[0]\r\n",
        "        epsilon = final_epsilon+((episodes-episode)*(initial_epsilon-final_epsilon)/episodes)\r\n",
        "        take_random_action = random.random()<=epsilon\r\n",
        "        if take_random_action:\r\n",
        "            print('random')\r\n",
        "            action = random.randint(0,1)\r\n",
        "        else:\r\n",
        "            action=torch.argmax(pred)\r\n",
        "\r\n",
        "        next_image, reward, endgame = current_game.nextframe(action)\r\n",
        "        #next_image = torch.from_numpy(np.expand_dims(np.transpose(next_image,(2,0,1)),0))\r\n",
        "        #next_image = next_image.float()\r\n",
        "        #next_state = torch.cat((state.squeeze(0)[3:,:,:], next_image))\r\n",
        "\r\n",
        "        next_image = pre_processing(next_image, 84,84)\r\n",
        "\r\n",
        "        #action = action.unsqueeze(0)\r\n",
        "        #reward = torch.from_numpy(np.array([reward],dtype=np.float32)).unsqueeze(0)\r\n",
        "        next_image = torch.from_numpy(next_image)\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            next_image = next_image.cuda()\r\n",
        "        next_state = torch.cat((state[0, 1:, :, :], next_image))[None, :, :, :]\r\n",
        "\r\n",
        "        replay_memory.append([state, action, reward, next_state, endgame])\r\n",
        "        if len(replay_memory) > replay_memory_size:\r\n",
        "            del replay_memory[0]\r\n",
        "        batch = random.sample(replay_memory, min(len(replay_memory), batch_size))\r\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = zip(*batch)\r\n",
        "        state_batch = torch.cat(tuple(state for state in state_batch))\r\n",
        "        action_batch = torch.from_numpy(\r\n",
        "            np.array([[1, 0] if action == 0 else [0, 1] for action in action_batch], dtype=np.float32))\r\n",
        "        reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None])\r\n",
        "        next_state_batch = torch.cat(tuple(state for state in next_state_batch))\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            state_batch = state_batch.cuda()\r\n",
        "            action_batch = action_batch.cuda()\r\n",
        "            reward_batch = reward_batch.cuda()\r\n",
        "            next_state_batch = next_state_batch.cuda()\r\n",
        "        \r\n",
        "        current_prediction_batch = model(state_batch)\r\n",
        "        next_prediction_batch = model(next_state_batch)\r\n",
        "\r\n",
        "        y_batch = torch.cat(\r\n",
        "            tuple(reward if terminal else reward + gamma * torch.max(prediction) for reward, terminal, prediction in\r\n",
        "                  zip(reward_batch, terminal_batch, next_prediction_batch)))\r\n",
        "        q_value = torch.sum(current_prediction_batch * action_batch, dim=1)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(q_value, y_batch)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        state = next_state\r\n",
        "        \r\n",
        "        if episode %50 == 0:\r\n",
        "            print(\"Episode: {}/{}, Action: {}, Loss: {}, Epsilon {}, Reward: {}, Q-value: {}\".format(\r\n",
        "            episode + 1, episodes, action, loss, epsilon, reward, torch.max(pred)))\r\n",
        "        episode+=1\r\n",
        "\r\n",
        "        if episode % 10000 == 0:\r\n",
        "            real_time_elapsed = time.time()-real_time_start\r\n",
        "            CPU_time_elapsed =time.process_time()-CPU_time_start\r\n",
        "            print(\"Real time elapsed : {}, CPU time elapsed : {}\".format(real_time_elapsed,CPU_time_elapsed))\r\n",
        "            checkpoint_path = \"checkpoint_{}.pt\".format(episode)\r\n",
        "            torch.save({\r\n",
        "            'checkpoint_episode': episode,\r\n",
        "            'model_state_dict': model.state_dict(),\r\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "            'loss':loss,\r\n",
        "            'real_time_elapsed' : real_time_elapsed,\r\n",
        "            'CPU_time_elapsed' : CPU_time_elapsed\r\n",
        "            }, checkpoint_path)\r\n",
        "\r\n",
        "\r\n",
        "train(1000000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}